{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare tweets and party programs\n",
    "\n",
    "https://github.com/adsieg/text_similarity/blob/master/Different%20Embeddings%20%2B%20Cosine%20Similarity%20%2B%20HeatMap%20illustration.ipynb \n",
    "\n",
    "https://www.machinelearningplus.com/nlp/cosine-similarity/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "programme = open('party_programmes/gruene.txt', \"r\").read()\n",
    "\n",
    "programme = programme.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning text \n",
    "\n",
    "#also stole this from: https://github.com/adsieg/text_similarity/blob/master/Different%20Embeddings%20%2B%20Cosine%20Similarity%20%2B%20HeatMap%20illustration.ipynb\n",
    "def preprocess(raw_text):\n",
    "\n",
    "    # keep only words\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "    # convert to lower case and split \n",
    "    words = letters_only_text.lower().split()\n",
    "\n",
    "    # remove stopwords\n",
    "    stopword_set = set(stopwords.words(\"german\"))\n",
    "    cleaned_words = list(set([w for w in words if w not in stopword_set]))\n",
    "\n",
    "    return cleaned_words\n",
    "\n",
    "gruene_cleaned = preprocess(programme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_comp = [gruene_cleaned, \"Karl Lauterbach soll Gesundheitsminister werden.\", \"Klimawandel ist ein wichtiges Thema\"] #comparig the program with two sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-6df5ba9fce2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcount_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## need to remove stopwords and use TfidfVectorizer instead??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msparse_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfor_comp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Create the Document Term Matrix\n",
    "#stole this code from here: https://www.machinelearningplus.com/nlp/cosine-similarity/  \n",
    "\n",
    "\n",
    "count_vectorizer = CountVectorizer() ## need to remove stopwords and use TfidfVectorizer instead?? \n",
    "sparse_matrix = count_vectorizer.fit_transform(for_comp)\n",
    "\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names(), \n",
    "                  index=['gruene', 'Lauterbach', 'Klima'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.06450307 0.06484987]\n",
      " [0.06450307 1.         0.        ]\n",
      " [0.06484987 0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pre-trained model\n",
    "\n",
    "# downloaded a German word embedding model from http://devmount.github.io/GermanWordEmbeddings/#download \n",
    "# das github von dem dude: https://github.com/devmount/GermanWordEmbeddings/blob/master/code/pca.ipynb\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"/Users/johannes/Downloads/german.model\", binary=True) # i know you guys didn't download it yet but just testing things. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I would need to do, to make this work:\n",
    "- Get a better embedding model for German language (given that this model doesn't even contain Baerbock)\n",
    "- Loop each word into the model and calculate the mean (or weighted mean)\n",
    "- then calculate the scipy distance \n",
    "- this can work. currently the main thing is the German Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26798216]], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(np.asarray(model[\"Gruene\"]).reshape(1,-1), np.asarray(model[\"Osten\"]).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying with the German FastText Wiki Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = fasttext.load_model(\"/Users/johannes/Downloads/wiki.de/wiki.de.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.10224563e-01,  9.07221064e-02,  2.96998650e-01,  9.94986519e-02,\n",
       "        4.12711173e-01,  3.06719720e-01,  3.04239661e-01, -1.50212556e-01,\n",
       "       -2.94918567e-03,  1.44883946e-01, -1.74862891e-01,  4.39091325e-02,\n",
       "        1.02864705e-01, -1.86042026e-01,  6.81281239e-02,  3.77346456e-01,\n",
       "        3.42065930e-01, -1.65252537e-01,  1.44632027e-01, -9.03984457e-02,\n",
       "        5.99124171e-02,  6.94528371e-02,  3.01870942e-01, -1.99355662e-01,\n",
       "        1.44362422e-02,  8.07003602e-02, -1.27326056e-01, -4.43713665e-02,\n",
       "       -2.21778452e-01, -1.40716955e-01,  2.43051007e-01,  3.54338884e-01,\n",
       "       -2.64221728e-01, -3.55228096e-01, -8.11479520e-03, -6.51899278e-01,\n",
       "        1.09513797e-01, -2.64682267e-02, -9.74089876e-02, -8.75184536e-02,\n",
       "        1.39009720e-02,  2.01167420e-01,  2.32282698e-01,  1.12420492e-01,\n",
       "        2.31429804e-02,  4.74265903e-01,  3.52653295e-01,  4.32717443e-01,\n",
       "        4.74041477e-02, -4.01689470e-01,  5.54402649e-01, -1.78509220e-01,\n",
       "       -1.61929339e-01, -2.00916022e-01, -1.79217815e-01, -9.25023928e-02,\n",
       "       -1.80713788e-01,  7.12386370e-02,  3.23159873e-01,  7.87794292e-02,\n",
       "       -2.19218526e-02, -4.87437211e-02,  3.26366425e-01,  2.21013948e-01,\n",
       "        2.16135178e-02,  1.58357784e-01, -3.21627520e-02,  8.62144530e-02,\n",
       "        2.38876306e-02,  3.45456183e-01,  2.37728283e-01,  4.85472605e-02,\n",
       "        8.72020349e-02,  2.75076896e-01,  4.98322770e-02, -1.09544940e-01,\n",
       "        5.77184884e-03, -2.39571780e-01, -8.25472102e-02, -4.99487549e-01,\n",
       "       -8.29978347e-01, -1.01877004e-01,  1.98185354e-01, -2.08640724e-01,\n",
       "       -1.60181820e-01, -3.30508709e-01, -1.83119491e-01,  4.76034313e-01,\n",
       "       -6.75499082e-01,  2.30178311e-01, -3.07661116e-01, -9.28787142e-02,\n",
       "       -1.48323905e-02, -6.70436621e-02, -2.39364758e-01, -2.02814683e-01,\n",
       "        1.31062225e-01,  5.16554825e-02, -4.79144484e-01,  2.63795316e-01,\n",
       "        3.83732840e-02, -4.41251934e-01,  4.26767021e-02,  2.43324921e-01,\n",
       "        1.33541122e-01, -3.25684935e-01,  1.56479478e-02,  1.86843216e-01,\n",
       "       -4.58996259e-02, -7.42012858e-02,  2.51403868e-01,  2.98984915e-01,\n",
       "       -3.36478874e-02,  1.03873447e-01,  1.28330626e-02, -3.09246257e-02,\n",
       "        5.46862721e-01,  3.74288887e-01,  6.53273542e-04, -3.71874481e-01,\n",
       "       -2.97680289e-01, -3.80814970e-01,  2.02733800e-01, -3.52982908e-01,\n",
       "       -2.17786655e-01, -3.65391970e-01,  1.96996778e-01, -1.67987928e-01,\n",
       "        3.81917171e-02,  3.56482230e-02, -5.43477014e-02, -2.07586363e-01,\n",
       "        1.06545992e-01,  2.13133916e-02,  1.91365078e-01,  4.88018930e-01,\n",
       "        2.02452034e-01,  2.60519441e-02,  4.09565747e-01, -2.04334736e-01,\n",
       "        4.03184369e-02, -2.48384774e-01, -2.65735000e-01,  2.61424445e-02,\n",
       "        2.59146571e-01, -1.49742603e-01,  2.28259772e-01, -1.02339685e-01,\n",
       "       -1.41023546e-01, -2.12500989e-01, -5.72342396e-01,  4.12143692e-02,\n",
       "        8.60858485e-02, -1.32022694e-01,  2.69259214e-02, -4.20224458e-01,\n",
       "       -2.59586960e-01,  2.78738104e-02,  3.30711633e-01,  3.17131996e-01,\n",
       "        3.81657660e-01, -4.08821255e-01,  8.73981118e-02,  3.00540864e-01,\n",
       "       -1.66610554e-02, -1.11325897e-01, -5.33075929e-01,  2.01191977e-01,\n",
       "       -1.43157318e-01,  1.67530134e-01, -4.62186068e-01,  1.47429422e-01,\n",
       "        2.86126256e-01, -1.80964336e-01, -1.58762977e-01, -1.07863024e-01,\n",
       "        3.02022845e-01, -3.72278661e-01,  2.99684882e-01,  2.48674974e-01,\n",
       "       -3.22151780e-01,  2.18441267e-03,  3.90923955e-02, -7.13191088e-03,\n",
       "        3.97443861e-01,  2.66786292e-02,  1.28904916e-02, -1.48775309e-01,\n",
       "        1.44811887e-02,  2.34916881e-01,  2.01385632e-01, -8.22166577e-02,\n",
       "        9.72306505e-02, -5.98557927e-02, -1.66612148e-01, -4.59800839e-01,\n",
       "        3.52968156e-01, -2.60414600e-01,  2.06618607e-01, -1.68373272e-01,\n",
       "       -7.83918146e-03,  7.27904439e-02,  7.70103112e-02, -1.23372450e-01,\n",
       "       -7.18310252e-02, -3.63854133e-02,  3.34128439e-02, -1.51220515e-01,\n",
       "        2.36552790e-01, -7.10785016e-02,  2.49837581e-02, -1.34615019e-01,\n",
       "        2.83134043e-01,  2.16301173e-01,  4.70056348e-02,  1.03293126e-02,\n",
       "        3.89564455e-01, -2.43126631e-01,  7.19136968e-02,  1.09015703e-01,\n",
       "        1.84792966e-01, -7.48406425e-02,  1.77303135e-01,  4.97412413e-01,\n",
       "       -2.27863416e-01,  3.12426209e-01,  2.81934559e-01,  2.21064359e-01,\n",
       "       -2.60904372e-01,  1.91452965e-01,  8.23787749e-02, -8.94062370e-02,\n",
       "        1.44134015e-01, -3.31997842e-01, -1.51021361e-01, -3.33488047e-01,\n",
       "       -3.88293982e-01, -5.33027127e-02, -8.90275314e-02, -1.11491524e-01,\n",
       "        1.78818002e-01,  2.26907045e-01, -3.27848196e-01, -1.81329027e-01,\n",
       "        3.18732150e-02,  3.57116133e-01, -3.82555902e-01,  2.78718948e-01,\n",
       "        1.11406088e-01,  5.08804806e-02,  1.20272329e-02, -6.04416318e-02,\n",
       "       -1.06485253e-02, -5.43046668e-02,  4.41562772e-01, -3.74904245e-01,\n",
       "        7.95891359e-02,  1.47498131e-01,  5.70889330e-03,  8.83169938e-03,\n",
       "       -1.38593186e-02,  1.41991079e-01,  3.89739349e-02,  1.26868337e-01,\n",
       "        5.30256191e-04,  4.81452905e-02,  3.15699488e-01,  1.99068412e-01,\n",
       "        8.06429163e-02, -9.96760800e-02,  1.77086860e-01,  3.40543658e-01,\n",
       "       -1.11947432e-01,  7.71693289e-02,  1.34229168e-01,  2.39032775e-01,\n",
       "        1.96971416e-01, -5.75753331e-01,  2.50477433e-01,  2.57825762e-01,\n",
       "       -3.52115691e-01, -2.56224662e-01,  1.16003286e-02,  1.77182272e-01,\n",
       "       -2.74152565e-03, -2.36184314e-01,  1.06209792e-01, -2.22324565e-01,\n",
       "        1.67444766e-01, -1.08260326e-01, -9.11668912e-02,  1.68940529e-01,\n",
       "       -2.79716700e-01,  3.10112357e-01,  9.90308896e-02,  2.01496616e-01,\n",
       "        5.90323396e-02,  5.00300765e-01, -8.91554914e-03,  1.85298458e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple test how a vector looks. it is pretty\n",
    "\n",
    "ft_model['Gruene']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_programm = gensim.utils.simple_preprocess(programme)\n",
    "cleaned_tweet = gensim.utils.simple_preprocess(\"Karl Lauterbach hat ne geile Fliege und die ist rot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#anscheinend hat dieses kack model keine ahnung wer Baerbock ist...\n",
    "program_vector = np.mean([ft_model[word] for word in cleaned_programm], axis = 0).reshape(1,-1)\n",
    "tweet_vector = np.mean([ft_model[word] for word in cleaned_tweet], axis = 0).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71076447]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(program_vector, tweet_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98a2b17e149e4e8a0b6acf04d01298b022c80a24b0767a95d910142ef5b4f869"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
