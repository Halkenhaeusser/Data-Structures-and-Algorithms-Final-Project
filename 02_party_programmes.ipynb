{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting text from Germany party programs\n",
    "\n",
    "How often do certain keywords come up in different party programmes? The goal here is a dataframe summarizing the different party programms.\n",
    "\n",
    "As a first step, we are importing the required packages reading in the data, cleaning it from stopwords, and plotting some relationships.\n",
    "\n",
    "- ``Tika`` is used to extract the content of each party's programme from the stored programme files in pdf format\n",
    "- ``Glob`` is used to detect each file that is stored in the party programme folder\n",
    "- ``Nltk`` can be used for various NLP tasks, here: Removing (German) stopwords as well as splitting up words\n",
    "- ``Matplotlib`` is used to create plots visualizing the similarity of Tweets and the party programmes' content\n",
    "- ``pandas`` is used for several dataframe transformations\n",
    "- ``collections`` is imported for the Counter packages to display how often each word appears in the programmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tika in c:\\programdata\\anaconda3\\lib\\site-packages (1.24)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from tika) (2.24.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tika) (50.3.1.post20201107)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->tika) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->tika) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->tika) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->tika) (3.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\carlo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Packages\n",
    "!pip install tika\n",
    "from tika import parser \n",
    "import glob\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing names of considered party programmes\n",
    "\n",
    "The glob library is used to print all files that are located in the `party_programmes`-folder inside of the Github repository. Beforehand, we downloaded each programme for all parties currently represented in the German parliament. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['party_programmes\\\\eyjg.pdf', 'party_programmes\\\\FDP_BTW2021_Wahlprogramm_1.pdf', 'party_programmes\\\\gruene.txt', 'party_programmes\\\\programm_der_partei_die_linke_erfurt2011_druckfassung2020.pdf', 'party_programmes\\\\Wahlprogramm-DIE-GRUENEN-Bundestagswahl-2021_barrierefrei.pdf']\n"
     ]
    }
   ],
   "source": [
    "print(glob.glob('party_programmes/*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the party programmes \n",
    "\n",
    "Here, all programs are stored in a new object. In the second step, the `parser.from.file`-function is used in order to create separate objects for each party's program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('party_programmes/*')\n",
    "\n",
    "\n",
    "spd_raw = parser.from_file('party_programmes/SPD-Zukunftsprogramm.pdf')\n",
    "cdu_raw = parser.from_file('party_programmes/eyjg.pdf')\n",
    "afd_raw = parser.from_file('party_programmes/20210611_AfD_Programm_2021.pdf')\n",
    "fdp_raw = parser.from_file('party_programmes/FDP_BTW2021_Wahlprogramm_1.pdf')\n",
    "linke_raw = parser.from_file('party_programmes/DIE_LINKE_Wahlprogramm_zur_Bundestagswahl_2021.pdf')\n",
    "gruene_raw = parser.from_file('party_programmes/Wahlprogramm-DIE-GRUENEN-Bundestagswahl-2021_barrierefrei.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First transformations\n",
    "\n",
    "In the next chunk, the programs are cleaned for further analysis: Linebreaks are removed, the `word_tokenize`-function is used to splitting up the text into words and German stopwords are removed. These steps are necessary for being able to analyze the programmes' content in a meaningful way, since stopwords do not add meaningful information to the programmes. Thus, removing them enables us to focus on the substantially meaningful information. Here, we are doing this exemplarily for the SPD programme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#remove linebreaks\n",
    "nonnewline = spd_raw['content'].strip()\n",
    "\n",
    "#separate the text into individual words\n",
    "text_tokens = word_tokenize(nonnewline)\n",
    "\n",
    "#delete all stopwords\n",
    "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words('german')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using the Count library to identify how many single words could be extracted form the programmes, excluding stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check out how many words we could inspect\n",
    "counted_words = Counter(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are able to manually specify words for which we want to display the frequency within the respective programmes. Here, we exemplarily the frequency of the word \"Klimawandel\" (climate change) in the SPD programme and identify that it appears three times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for each word we can access the number of words we got\n",
    "counted_words[\"Klimawandels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=open(r\"party_programmes/gruene.txt\",\"a\")\n",
    "file1.writelines(gruene_raw['content'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "acf825a93f1d67e9b21f3d7be3de1b7301682f1e83d4489e56181e84878ca65e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
